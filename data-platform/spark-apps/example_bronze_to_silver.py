#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Bronze to Silver ë³€í™˜ ì˜ˆì‹œ

ë¸Œë¡ ì¦ˆ ë ˆì´ì–´ì˜ ì›ì²œ ë°ì´í„°ë¥¼ ì½ì–´ ìŠ¤í‚¤ë§ˆ ì •ì˜ì— ë”°ë¼ ì‹¤ë²„ í…Œì´ë¸”ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ì‹¤ë²„ í…Œì´ë¸”:
1. silver_usage - ì‚¬ìš©ëŸ‰ ë°ì´í„°
2. silver_contacts - ë‹´ë‹¹ì ì •ë³´  
3. silver_contracts - ê³„ì•½ ì •ë³´
"""

from __future__ import annotations

import argparse
import os
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import boto3
from botocore.client import Config
from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql.functions import (
    col, regexp_replace, when, lit, to_date,
    trim, upper, coalesce, row_number, desc,
    max as spark_max
)
from pyspark.sql.types import StringType, DateType


# -----------------------------------------------------------------------------
# í™˜ê²½ ê°ì§€ ë° ì„¤ì •
# -----------------------------------------------------------------------------
def is_running_in_docker() -> bool:
    """ì»¨í…Œì´ë„ˆ í™˜ê²½(Docker/K8s) ì‹¤í–‰ ì—¬ë¶€ ê°ì§€"""
    if os.path.exists("/.dockerenv"):
        return True
    return os.getenv("RUNNING_IN_DOCKER", "").lower() in ("1", "true", "yes")


def get_default_s3_endpoint() -> str:
    """ì‹¤í–‰ í™˜ê²½ì— ë§ì¶˜ ê¸°ë³¸ S3(MinIO) ì—”ë“œí¬ì¸íŠ¸ ê²°ì •"""
    return "http://minio:9000" if is_running_in_docker() else "http://localhost:9000"


def create_spark_session(app_name: str) -> SparkSession:
    """Spark ì„¸ì…˜ ìƒì„± - Submitì—ì„œ ë„˜ê¸´ conf/env ì‚¬ìš©"""
    builder = SparkSession.builder.appName(app_name)
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("INFO")
    return spark


def setup_s3_client(target_bucket: str | None = None):
    """S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
    s3_endpoint = os.getenv("S3_ENDPOINT") or get_default_s3_endpoint()
    access_key = os.getenv("MINIO_ROOT_USER", "admin")
    secret_key = os.getenv("MINIO_ROOT_PASSWORD", "password")

    s3_client = boto3.client(
        "s3",
        endpoint_url=s3_endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        config=Config(signature_version="s3v4"),
        region_name="us-east-1",
    )
    return s3_client


# -----------------------------------------------------------------------------
# ë°ì´í„° ì •ì œ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def safe_count(df: DataFrame, label: str = "") -> Optional[int]:
    """ì•ˆì „í•œ count() ë˜í¼: ì‹¤íŒ¨ ì‹œ None ë°˜í™˜"""
    try:
        return df.count()
    except Exception as e:
        print(f"  âš ï¸ [{label}] ê±´ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return None


def clean_usage_data(spark: SparkSession, df: DataFrame) -> DataFrame:
    """ì‚¬ìš©ëŸ‰ ë°ì´í„° ì •ì œ -> silver_usage"""
    print("ğŸ”§ ì‚¬ìš©ëŸ‰ ë°ì´í„° ì •ì œ ì‹œì‘")
    print(f"  ğŸ“Š ì›ë³¸ ë°ì´í„°: {safe_count(df, 'usage')}í–‰")

    # ìŠ¤í‚¤ë§ˆ ì •ì˜ì— ë”°ë¥¸ ì»¬ëŸ¼ ë§¤í•‘ ë° ë³€í™˜
    df_cleaned = df.select([
        # ë‚ ì§œ ë³€í™˜: '25. 07. 31' -> '2025-07'
        regexp_replace(
            regexp_replace(col("ë°ì´í„°_ì¶”ì¶œì¼ì"), r"^(\d{2})\.\s*(\d{1,2})\.\s*\d{1,2}$", r"20$1-$2"),
            r"-(\d)$", r"-0$1"
        ).cast(StringType()).alias("usage_month"),

        # ë„ë©”ì¸ëª… ì •ì œ
        trim(col("ë„ë©”ì¸ëª…")).cast(StringType()).alias("domain"),

        # ìˆ«ì í•„ë“œ ì •ì œ: ì‰¼í‘œ ì œê±° í›„ íƒ€ì… ë³€í™˜
        regexp_replace(col("ë©”ì¼ê°œìˆ˜"), ",", "").cast("int").alias("mail_count"),
        regexp_replace(col("ìš©ëŸ‰_Bytes"), ",", "").cast("long").alias("usage_bytes"),
        regexp_replace(col("ì‚¬ìš©ììˆ˜_active"), ",", "").cast("int").alias("active_users"),
    ])

    # Null ê°’ ë° ë¹ˆ ë¬¸ìì—´ í•„í„°ë§
    df_cleaned = df_cleaned.filter(
        col("domain").isNotNull() &
        (col("domain") != "") &
        col("usage_month").isNotNull()
    )

    # ì›”Ã—ë„ë©”ì¸ ê¸°ì¤€ ì¤‘ë³µ ì œê±° (maxë¡œ ì •ê·œí™”)
    df_cleaned = (
        df_cleaned
        .groupBy("usage_month", "domain")
        .agg(
            spark_max("mail_count").alias("mail_count"),
            spark_max("usage_bytes").alias("usage_bytes"),
            spark_max("active_users").alias("active_users"),
        )
    )

    print(f"  âœ… ì •ì œ ì™„ë£Œ: {safe_count(df_cleaned, 'usage')}í–‰")
    return df_cleaned


def clean_contracts_data(spark: SparkSession, df: DataFrame) -> DataFrame:
    """ê³„ì•½ ì •ë³´ ì •ì œ -> silver_contracts"""
    print("ğŸ”§ ê³„ì•½ ì •ë³´ ì •ì œ ì‹œì‘")
    print(f"  ğŸ“Š ì›ë³¸ ë°ì´í„°: {safe_count(df, 'contracts')}í–‰")

    # ì»¬ëŸ¼ëª…ì—ì„œ ì¤„ë°”ê¿ˆ ë¬¸ì ì œê±°
    for old_col in df.columns:
        new_col = old_col.replace('\n', '').replace('\r', '')
        if old_col != new_col:
            df = df.withColumnRenamed(old_col, new_col)

    df_cleaned = df.select([
        # ID ì²˜ë¦¬
        when(col("ë²ˆí˜¸").rlike(r"^\d+$"), col("ë²ˆí˜¸").cast("int")).alias("id"),

        # ê³„ì•½ ìƒíƒœ ê²°ì •
        when(col("ë²ˆí˜¸") == "ì¢…ë£Œ", "TERMINATED")
        .when(col("ë²ˆí˜¸") == "ì˜ˆì •", "SCHEDULED")
        .otherwise("ACTIVE").alias("contract_status"),

        # ê¸°ë³¸ ì •ë³´
        trim(col("ê³ ê°ëª…")).cast(StringType()).alias("customer_name"),
        trim(col("ë„ë©”ì¸ëª…")).cast(StringType()).alias("domain"),
        trim(col("ê³µê³µë¶„ë¥˜")).cast(StringType()).alias("public_sector_type"),
        trim(col("CSP")).cast(StringType()).alias("csp"),

        # ìš©ëŸ‰ ë° ì‚¬ìš©ì ìˆ˜
        regexp_replace(coalesce(col("ê³„ì•½_ìš©ëŸ‰_GB"), lit("0")), ",", "").cast("int").alias("contract_gb"),
        regexp_replace(coalesce(col("ê³„ì•½_USER"), lit("0")), ",", "").cast("int").alias("contract_users"),

        # ë‚ ì§œ í•„ë“œ ì •ì œ
        to_date(
            regexp_replace(coalesce(col("ìµœì´ˆ_ì˜¤í”ˆì¼"), lit("")), r"(\d{2})\.\s*(\d{1,2})\.\s*(\d{1,2})", r"20$1-$2-$3"),
            "yyyy-M-d"
        ).alias("open_date"),
    ])

    # ìœ íš¨í•œ ê³„ì•½ë§Œ í•„í„°ë§
    df_cleaned = df_cleaned.filter(
        col("domain").isNotNull() &
        (col("domain") != "") &
        col("customer_name").isNotNull() &
        (col("customer_name") != "")
    )

    print(f"  âœ… ì •ì œ ì™„ë£Œ: {safe_count(df_cleaned, 'contracts')}í–‰")
    return df_cleaned


# -----------------------------------------------------------------------------
# Iceberg ì €ì¥
# -----------------------------------------------------------------------------
def save_to_silver(
    spark: SparkSession,
    df: DataFrame,
    table_name: str,
    silver_bucket: str,
) -> None:
    """ì‹¤ë²„ í…Œì´ë¸”ì„ Iceberg í…Œì´ë¸”ë¡œ ì €ì¥"""
    print(f"ğŸ’¾ ì‹¤ë²„ í…Œì´ë¸” ì €ì¥: {table_name}")

    namespace = "silver"
    target_table = f"iceberg.{namespace}.{table_name}"

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS iceberg.{namespace}")

    # í…Œì´ë¸”ë³„ íŒŒí‹°ì…”ë‹ ì „ëµ
    if table_name == "silver_usage":
        partition_cols = ["usage_month"]
    else:
        # ingest_date íŒŒí‹°ì…”ë‹
        current_date_str = datetime.now().strftime("%Y-%m-%d")
        df = df.withColumn("ingest_date", to_date(lit(current_date_str)))
        partition_cols = ["ingest_date"]

    # í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ìƒì„±/ì¶”ê°€
    if spark.catalog.tableExists(target_table):
        print("  ğŸ”„ ê¸°ì¡´ í…Œì´ë¸”ì— íŒŒí‹°ì…˜ ë®ì–´ì“°ê¸°")
        df.writeTo(target_table).overwritePartitions()
    else:
        print("  ğŸ†• ìƒˆ í…Œì´ë¸” ìƒì„±")
        writer = df.writeTo(target_table)
        for col_name in partition_cols:
            writer = writer.partitionedBy(col_name)
        writer.create()

    print(f"  âœ… ì €ì¥ ì™„ë£Œ: {table_name}")


# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
def run_bronze_to_silver_job(
    bronze_bucket: str,
    silver_bucket: str,
    dataset: str = "example",
) -> None:
    """ë¸Œë¡ ì¦ˆ to ì‹¤ë²„ ë³€í™˜ ë©”ì¸ ì‘ì—…"""
    print("=" * 60)
    print("ğŸš€ Bronze to Silver ë³€í™˜ ì‹œì‘")
    print(f"  - Bronze ë²„í‚·: {bronze_bucket}")
    print(f"  - Silver ë²„í‚·: {silver_bucket}")
    print(f"  - ë°ì´í„°ì…‹: {dataset}")
    print("=" * 60)

    spark = create_spark_session(app_name="Bronze-to-Silver")

    # ë¸Œë¡ ì¦ˆ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°
    usage_df = spark.table(f"iceberg.bronze.{dataset}_usage")
    contracts_df = spark.table(f"iceberg.bronze.{dataset}_contracts")

    # ë°ì´í„° ì •ì œ
    silver_usage = clean_usage_data(spark, usage_df)
    silver_contracts = clean_contracts_data(spark, contracts_df)

    # ì‹¤ë²„ í…Œì´ë¸” ì €ì¥
    save_to_silver(spark, silver_usage, f"{dataset}_silver_usage", silver_bucket)
    save_to_silver(spark, silver_contracts, f"{dataset}_silver_contracts", silver_bucket)

    print("=" * 60)
    print("âœ… Bronze to Silver ë³€í™˜ ì™„ë£Œ")
    print("=" * 60)
    spark.stop()


def parse_args():
    parser = argparse.ArgumentParser(description="Bronze to Silver ETL")
    parser.add_argument("--bronze-bucket", required=True, help="Bronze data bucket")
    parser.add_argument("--silver-bucket", required=True, help="Silver data bucket")
    parser.add_argument("--dataset", default="example", help="Dataset name")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    run_bronze_to_silver_job(
        args.bronze_bucket,
        args.silver_bucket,
        args.dataset,
    )
