#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Silver to Gold ë³€í™˜ ì˜ˆì‹œ

ì‹¤ë²„ ë ˆì´ì–´ì˜ ì •ì œëœ ë°ì´í„°ë¥¼ ì½ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ í…Œì´ë¸”(ê³¨ë“œ)ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ê³¨ë“œ í…Œì´ë¸”:
1. gold_customer_master - ê³ ê° ë§ˆìŠ¤í„° (í†µí•© ê³ ê° ì •ë³´)
2. gold_usage_analytics - ì‚¬ìš©ëŸ‰ ë¶„ì„ (ì›”ë³„ íŠ¸ë Œë“œ, ì¦ê°ë¥ )
3. gold_revenue_analytics - ë§¤ì¶œ ë¶„ì„ (ê³ ê°ë³„/ì„œë¹„ìŠ¤ë³„ ë§¤ì¶œ)
"""

from __future__ import annotations

import argparse
import os
from datetime import datetime
from typing import Optional

from pyspark.sql import SparkSession, DataFrame, Window
from pyspark.sql.functions import (
    col, lit, to_date, coalesce,
    sum as spark_sum, avg as spark_avg, max as spark_max, min as spark_min,
    count, row_number, desc, dense_rank,
    when, lag, round as spark_round,
    concat_ws, collect_list
)
from pyspark.sql.types import StringType


# -----------------------------------------------------------------------------
# Spark ì„¸ì…˜ ì„¤ì •
# -----------------------------------------------------------------------------
def create_spark_session(app_name: str) -> SparkSession:
    """Spark ì„¸ì…˜ ìƒì„±"""
    builder = SparkSession.builder.appName(app_name)
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("INFO")
    return spark


# -----------------------------------------------------------------------------
# ê³¨ë“œ í…Œì´ë¸” ìƒì„± í•¨ìˆ˜
# -----------------------------------------------------------------------------
def create_customer_master(
    spark: SparkSession,
    contracts_df: DataFrame,
    contacts_df: DataFrame,
    usage_df: DataFrame,
) -> DataFrame:
    """ê³ ê° ë§ˆìŠ¤í„° í…Œì´ë¸” ìƒì„±
    
    ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬ ê³ ê°ë³„ í•µì‹¬ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ êµ¬ì„±
    """
    print("ğŸ—ï¸ ê³ ê° ë§ˆìŠ¤í„° í…Œì´ë¸” ìƒì„±")

    # ê³„ì•½ ì •ë³´ì—ì„œ ê³ ê°ë³„ ìµœì‹  ê³„ì•½ ì¶”ì¶œ
    contract_window = Window.partitionBy("customer_name").orderBy(desc("contract_date"))
    latest_contracts = (
        contracts_df
        .withColumn("rn", row_number().over(contract_window))
        .filter(col("rn") == 1)
        .drop("rn")
        .select(
            col("customer_name"),
            col("domain"),
            col("contract_status"),
            col("csp"),
            col("public_sector_type"),
            col("contract_gb"),
            col("contract_users"),
            col("contract_date"),
            col("open_date"),
        )
    )

    # ë‹´ë‹¹ì ì •ë³´ ì§‘ê³„ (ê³ ê°ë³„ ë‹´ë‹¹ì ìˆ˜)
    contact_counts = (
        contacts_df
        .groupBy("domain")
        .agg(
            count("*").alias("total_contacts"),
            spark_sum(when(col("is_current_contact") == True, 1).otherwise(0)).alias("active_contacts"),
        )
    )

    # ì‚¬ìš©ëŸ‰ ì •ë³´ ì§‘ê³„ (ìµœê·¼ ì›” ê¸°ì¤€)
    usage_window = Window.partitionBy("domain").orderBy(desc("usage_month"))
    latest_usage = (
        usage_df
        .withColumn("rn", row_number().over(usage_window))
        .filter(col("rn") == 1)
        .drop("rn")
        .select(
            col("domain"),
            col("usage_month").alias("latest_usage_month"),
            col("usage_bytes").alias("latest_usage_bytes"),
            col("active_users").alias("latest_active_users"),
        )
    )

    # ì¡°ì¸í•˜ì—¬ ë§ˆìŠ¤í„° í…Œì´ë¸” êµ¬ì„±
    customer_master = (
        latest_contracts
        .join(contact_counts, "domain", "left")
        .join(latest_usage, "domain", "left")
        .select(
            col("customer_name"),
            col("domain"),
            col("contract_status"),
            col("csp"),
            col("public_sector_type"),
            col("contract_gb"),
            col("contract_users"),
            col("open_date"),
            col("contract_date"),
            coalesce(col("total_contacts"), lit(0)).alias("total_contacts"),
            coalesce(col("active_contacts"), lit(0)).alias("active_contacts"),
            col("latest_usage_month"),
            col("latest_usage_bytes"),
            col("latest_active_users"),
            lit(datetime.now().strftime("%Y-%m-%d")).alias("snapshot_date"),
        )
    )

    print(f"  âœ… ê³ ê° ë§ˆìŠ¤í„° ìƒì„± ì™„ë£Œ: {customer_master.count()}ê°œ ê³ ê°")
    return customer_master


def create_usage_analytics(
    spark: SparkSession,
    usage_df: DataFrame,
) -> DataFrame:
    """ì‚¬ìš©ëŸ‰ ë¶„ì„ í…Œì´ë¸” ìƒì„±
    
    ì›”ë³„ íŠ¸ë Œë“œ, ì „ì›” ëŒ€ë¹„ ì¦ê°ë¥  ê³„ì‚°
    """
    print("ğŸ“Š ì‚¬ìš©ëŸ‰ ë¶„ì„ í…Œì´ë¸” ìƒì„±")

    # ì›”ë³„ ìœˆë„ìš° ì •ì˜
    monthly_window = Window.partitionBy("domain").orderBy("usage_month")

    usage_analytics = (
        usage_df
        .withColumn("prev_usage_bytes", lag("usage_bytes", 1).over(monthly_window))
        .withColumn("prev_active_users", lag("active_users", 1).over(monthly_window))
        .withColumn(
            "usage_growth_rate",
            when(
                col("prev_usage_bytes").isNotNull() & (col("prev_usage_bytes") > 0),
                spark_round(
                    (col("usage_bytes") - col("prev_usage_bytes")) / col("prev_usage_bytes") * 100,
                    2
                )
            ).otherwise(None)
        )
        .withColumn(
            "user_growth_rate",
            when(
                col("prev_active_users").isNotNull() & (col("prev_active_users") > 0),
                spark_round(
                    (col("active_users") - col("prev_active_users")) / col("prev_active_users") * 100,
                    2
                )
            ).otherwise(None)
        )
        .select(
            col("domain"),
            col("usage_month"),
            col("usage_bytes"),
            col("active_users"),
            col("mail_count"),
            col("usage_growth_rate"),
            col("user_growth_rate"),
            # ìš©ëŸ‰ ë“±ê¸‰ ë¶„ë¥˜
            when(col("usage_bytes") >= 1e12, "Enterprise")
            .when(col("usage_bytes") >= 1e11, "Large")
            .when(col("usage_bytes") >= 1e10, "Medium")
            .otherwise("Small").alias("usage_tier"),
        )
    )

    print(f"  âœ… ì‚¬ìš©ëŸ‰ ë¶„ì„ ìƒì„± ì™„ë£Œ: {usage_analytics.count()}í–‰")
    return usage_analytics


def create_revenue_analytics(
    spark: SparkSession,
    contracts_df: DataFrame,
) -> DataFrame:
    """ë§¤ì¶œ ë¶„ì„ í…Œì´ë¸” ìƒì„±
    
    ê³ ê°ë³„/ì„œë¹„ìŠ¤ë³„ ë§¤ì¶œ ì§‘ê³„
    """
    print("ğŸ’° ë§¤ì¶œ ë¶„ì„ í…Œì´ë¸” ìƒì„±")

    # CSPë³„ ë§¤ì¶œ ì§‘ê³„
    revenue_by_csp = (
        contracts_df
        .filter(col("contract_status") == "ACTIVE")
        .groupBy("csp")
        .agg(
            count("*").alias("customer_count"),
            spark_sum("contract_gb").alias("total_contract_gb"),
            spark_sum("contract_users").alias("total_contract_users"),
            spark_avg("contract_gb").alias("avg_contract_gb"),
        )
    )

    # ê³µê³µë¶„ë¥˜ë³„ ë§¤ì¶œ ì§‘ê³„
    revenue_by_sector = (
        contracts_df
        .filter(col("contract_status") == "ACTIVE")
        .groupBy("public_sector_type")
        .agg(
            count("*").alias("customer_count"),
            spark_sum("contract_gb").alias("total_contract_gb"),
            spark_sum("contract_users").alias("total_contract_users"),
        )
    )

    # ê³ ê°ë³„ ë§¤ì¶œ ìˆœìœ„
    revenue_window = Window.orderBy(desc("contract_gb"))
    customer_revenue_rank = (
        contracts_df
        .filter(col("contract_status") == "ACTIVE")
        .withColumn("revenue_rank", dense_rank().over(revenue_window))
        .select(
            col("customer_name"),
            col("domain"),
            col("csp"),
            col("contract_gb"),
            col("contract_users"),
            col("revenue_rank"),
            when(col("revenue_rank") <= 10, "Top 10")
            .when(col("revenue_rank") <= 50, "Top 50")
            .otherwise("Others").alias("revenue_tier"),
        )
    )

    print(f"  âœ… ë§¤ì¶œ ë¶„ì„ ìƒì„± ì™„ë£Œ")
    return customer_revenue_rank


# -----------------------------------------------------------------------------
# ì €ì¥ í•¨ìˆ˜
# -----------------------------------------------------------------------------
def save_to_gold(
    spark: SparkSession,
    df: DataFrame,
    table_name: str,
) -> None:
    """ê³¨ë“œ í…Œì´ë¸” ì €ì¥"""
    print(f"ğŸ’¾ ê³¨ë“œ í…Œì´ë¸” ì €ì¥: {table_name}")

    namespace = "gold"
    target_table = f"iceberg.{namespace}.{table_name}"

    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS iceberg.{namespace}")

    # ìŠ¤ëƒ…ìƒ· ë‚ ì§œë¡œ íŒŒí‹°ì…”ë‹
    current_date_str = datetime.now().strftime("%Y-%m-%d")
    df_with_partition = df.withColumn("snapshot_date", to_date(lit(current_date_str)))

    # í…Œì´ë¸” ì €ì¥
    if spark.catalog.tableExists(target_table):
        df_with_partition.writeTo(target_table).overwritePartitions()
    else:
        df_with_partition.writeTo(target_table).partitionedBy("snapshot_date").create()

    print(f"  âœ… ì €ì¥ ì™„ë£Œ: {table_name}")


# -----------------------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
def run_silver_to_gold_job(
    dataset: str = "example",
) -> None:
    """Silver to Gold ë³€í™˜ ë©”ì¸ ì‘ì—…"""
    print("=" * 60)
    print("ğŸš€ Silver to Gold ë³€í™˜ ì‹œì‘")
    print(f"  - ë°ì´í„°ì…‹: {dataset}")
    print("=" * 60)

    spark = create_spark_session(app_name="Silver-to-Gold")

    # ì‹¤ë²„ í…Œì´ë¸”ì—ì„œ ë°ì´í„° ì½ê¸°
    contracts_df = spark.table(f"iceberg.silver.{dataset}_silver_contracts")
    contacts_df = spark.table(f"iceberg.silver.{dataset}_silver_contacts")
    usage_df = spark.table(f"iceberg.silver.{dataset}_silver_usage")

    # ê³¨ë“œ í…Œì´ë¸” ìƒì„±
    customer_master = create_customer_master(spark, contracts_df, contacts_df, usage_df)
    usage_analytics = create_usage_analytics(spark, usage_df)
    revenue_analytics = create_revenue_analytics(spark, contracts_df)

    # ê³¨ë“œ í…Œì´ë¸” ì €ì¥
    save_to_gold(spark, customer_master, f"{dataset}_gold_customer_master")
    save_to_gold(spark, usage_analytics, f"{dataset}_gold_usage_analytics")
    save_to_gold(spark, revenue_analytics, f"{dataset}_gold_revenue_analytics")

    print("=" * 60)
    print("âœ… Silver to Gold ë³€í™˜ ì™„ë£Œ")
    print("=" * 60)
    spark.stop()


def parse_args():
    parser = argparse.ArgumentParser(description="Silver to Gold ETL")
    parser.add_argument("--dataset", default="example", help="Dataset name")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    run_silver_to_gold_job(args.dataset)
